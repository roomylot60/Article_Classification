import requests
from bs4 import BeautifulSoup

# ë„¤ì´ë²„ ë‰´ìŠ¤ ì„¹ì…˜ë³„ URL ë§¤í•‘
SECTION_URLS = {
    "ì •ì¹˜": "https://news.naver.com/section/100",
    "ê²½ì œ": "https://news.naver.com/section/101",
    "ì‚¬íšŒ": "https://news.naver.com/section/102",
    "ìƒí™œ": "https://news.naver.com/section/103",
    "ì„¸ê³„": "https://news.naver.com/section/104",
    "IT": "https://news.naver.com/section/105"
}

# âœ… íŠ¹ì • ì„¹ì…˜ì—ì„œ ê¸°ì‚¬ URL ê°€ì ¸ì˜¤ê¸° (num_articles ê°œìˆ˜ ì •í™•íˆ ìœ ì§€)
def get_article_urls(section, num_articles=10):
    if section not in SECTION_URLS:
        print(f"âŒ ì§€ì›í•˜ì§€ ì•ŠëŠ” ì„¹ì…˜ì…ë‹ˆë‹¤: {section}")
        return []

    url = SECTION_URLS[section]
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0"}

    response = requests.get(url, headers=headers)
    if response.status_code != 200:
        print(f"âŒ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")
        return []

    soup = BeautifulSoup(response.text, "html.parser")
    
    # âœ… ëª¨ë“  ê¸°ì‚¬ ë§í¬ ê°€ì ¸ì˜¤ê¸°
    article_urls = []
    for a in soup.select("div.sa_text a"):
        if "href" in a.attrs:
            href = a["href"]
            # âœ… "/cluster/"ê°€ í¬í•¨ëœ URLì€ ì œì™¸
            if "/cluster/" in href:
                continue
            # âœ… "/comment/"ê°€ í¬í•¨ëœ URLì€ ì •ìƒ ê¸°ì‚¬ URLë¡œ ë³€í™˜
            if "/comment/" in href:
                href = href.replace("/comment/", "/")

            article_urls.append(href)

            # âœ… ìš”ì²­í•œ ê¸°ì‚¬ ê°œìˆ˜ë¥¼ ì¶©ì¡±í•˜ë©´ ì¤‘ë‹¨
            if len(article_urls) >= num_articles:
                break

    return article_urls

# âœ… í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    section = "ì •ì¹˜"  # ì˜ˆì œ: ì •ì¹˜ ì„¹ì…˜ ê¸°ì‚¬ 10ê°œ ê°€ì ¸ì˜¤ê¸°
    article_urls = get_article_urls(section, num_articles=10)
    
    print(f"ğŸ“Œ {section} ì„¹ì…˜ ê¸°ì‚¬ URL ëª©ë¡:")
    for idx, url in enumerate(article_urls, start=1):
        print(f"{idx}. {url}")
